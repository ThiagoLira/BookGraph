{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('annotated/dataset_annotated-fixed-done.jsonl','r') as f:\n",
    "    \n",
    "    ds_calibre = list(map(lambda x : json.loads(x),f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotated/smaller_dataset_goodreads(ANNOTATED).jsonl','r') as f:\n",
    "    \n",
    "    ds_goodreads = list(map(lambda x : json.loads(x),f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotated/odyssey_annotaded.jsonl','r') as f:\n",
    "    \n",
    "    ds_cliff = list(map(lambda x : json.loads(x),f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goodreads dataset needs preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in ds_goodreads:\n",
    "    book = d['meta']['book']\n",
    "    \n",
    "    temp = d['text'].replace('[[BOOK]]',book)\n",
    "    d['text'] = temp\n",
    "    b = temp.find(book)\n",
    "    e = b + len(book)\n",
    "\n",
    "    #dict(start=5,end=14,text=\"Tal Perry\",label=\"Person\")\n",
    "    entities = {\n",
    "        'start' : b,\n",
    "        'end' : e,\n",
    "        'text' : book,\n",
    "        'label' : \"OBRA\",\n",
    "        'tag' : \"OBRA\"\n",
    "    }\n",
    "    if(d['labels']==['negative']):\n",
    "        d['entities'] = []\n",
    "    else:\n",
    "        d['entities'] = [entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_goodreads.jsonl','r') as f:\n",
    "    \n",
    "    ds_goodreads_not_annotated = list(map(lambda x : json.loads(x),f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = list(set([d['meta']['book'] for d in ds_goodreads_not_annotated]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Untouchable',\n",
       " 'Love Poems',\n",
       " 'The Untouchables',\n",
       " 'Ludwig Wittgenstein',\n",
       " 'Rebellion',\n",
       " 'Angels',\n",
       " 'Hideaway',\n",
       " 'Moby Dick',\n",
       " 'Velocity',\n",
       " 'Moll Flanders']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change books in a dataset to other random books, so the NER model doesn't memorize a specific book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "for d in ds_cliff:\n",
    "    book_to_change = np.random.choice(books,1)[0]\n",
    "    book = 'The Odyssey'\n",
    "    \n",
    "    if (d['labels']!=[['negative']]):\n",
    "        temp = d['text'].replace(book,book_to_change)\n",
    "        d['text'] = temp\n",
    "        b = temp.find(book_to_change)\n",
    "        e = b + len(book_to_change)\n",
    "\n",
    "        #dict(start=5,end=14,text=\"Tal Perry\",label=\"Person\")\n",
    "        entities = {\n",
    "            'start' : b,\n",
    "            'end' : e,\n",
    "            'text' : book_to_change,\n",
    "            'label' : \"OBRA\",\n",
    "            'tag' : \"OBRA\"\n",
    "        }\n",
    "\n",
    "        d['meta']['book'] = book_to_change\n",
    "    \n",
    "    if(d['labels']==[['negative']]):\n",
    "        d['entities'] = []\n",
    "    else:\n",
    "        d['entities'] = [entities]\n",
    "        \n",
    "# delete entries without tags\n",
    "#for i in range(len(ds_cliff)):\n",
    "    #if(ds_cliff[i]['labels']==[['negative']]):\n",
    "        #del ds_cliff[i]\n",
    "        \n",
    "for d in ds_calibre:\n",
    "    \n",
    "    if d['entities']:\n",
    "        book_to_change = np.random.choice(books,1)[0]\n",
    "        book = d['entities'][0]['text']\n",
    "        \n",
    "        temp = d['text'].replace(book,book_to_change)\n",
    "        d['text'] = temp\n",
    "        b = temp.find(book_to_change)\n",
    "        e = b + len(book_to_change)\n",
    "\n",
    "        #dict(start=5,end=14,text=\"Tal Perry\",label=\"Person\")\n",
    "        entities = {\n",
    "            'start' : b,\n",
    "            'end' : e,\n",
    "            'text' : book_to_change,\n",
    "            'label' : \"OBRA\",\n",
    "            'tag' : \"OBRA\"\n",
    "        }\n",
    "\n",
    "        d['meta']['book'] = book_to_change\n",
    "\n",
    "        d['entities'] = [entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.extend(ds_calibre)\n",
    "ds.extend(ds_cliff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(ds)\n",
    "\n",
    "dataset_train = ds[:-50]\n",
    "dataset_test = ds[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('ner_datasets/dataset_annotated_ner_calibre.jsonl','w') as f:\n",
    "#    for js in ds: \n",
    "#        f.write(json.dumps(js) + '\\n')\n",
    "#    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert ENTITIES to Hugginface Format (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lira/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List,Any\n",
    "IntList = List[int] # A list of token_ids\n",
    "IntListList = List[IntList] # A List of List of token_ids, e.g. a Batch\n",
    "\n",
    "def align_tokens_and_annotations_bilou(tokenized, annotations):\n",
    "    tokens = tokenized.tokens\n",
    "    aligned_labels = [\"O\"] * len(\n",
    "        tokens\n",
    "    )  # Make a list to store our labels the same length as our tokens\n",
    "    for anno in annotations:\n",
    "        annotation_token_ix_set = (\n",
    "            set()\n",
    "        )  # A set that stores the token indices of the annotation\n",
    "        for char_ix in range(anno[\"start\"], anno[\"end\"]):\n",
    "\n",
    "            token_ix = tokenized.char_to_token(char_ix)\n",
    "            if token_ix is not None:\n",
    "                annotation_token_ix_set.add(token_ix)\n",
    "        if len(annotation_token_ix_set) == 1:\n",
    "            # If there is only one token\n",
    "            token_ix = annotation_token_ix_set.pop()\n",
    "            prefix = (\n",
    "                \"U\"  # This annotation spans one token so is prefixed with U for unique\n",
    "            )\n",
    "            aligned_labels[token_ix] = f\"{prefix}-{anno['label']}\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            last_token_in_anno_ix = len(annotation_token_ix_set) - 1\n",
    "            for num, token_ix in enumerate(sorted(annotation_token_ix_set)):\n",
    "                if num == 0:\n",
    "                    prefix = \"B\"\n",
    "                elif num == last_token_in_anno_ix:\n",
    "                    prefix = \"L\"  # Its the last token\n",
    "                else:\n",
    "                    prefix = \"I\"  # We're inside of a multi token annotation\n",
    "                aligned_labels[token_ix] = f\"{prefix}-{anno['label']}\"\n",
    "    return aligned_labels\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "class LabelSet:\n",
    "    def __init__(self, labels: List[str]):\n",
    "        self.labels_to_id = {}\n",
    "        self.ids_to_label = {}\n",
    "        self.labels_to_id[\"O\"] = 0\n",
    "        self.ids_to_label[0] = \"O\"\n",
    "        num = 0  # in case there are no labels\n",
    "        # Writing BILU will give us incremntal ids for the labels\n",
    "        for _num, (label, s) in enumerate(itertools.product(labels, \"BILU\")):\n",
    "            num = _num + 1  # skip 0\n",
    "            l = f\"{s}-{label}\"\n",
    "            self.labels_to_id[l] = num\n",
    "            self.ids_to_label[num] = l\n",
    "        # Add the OUTSIDE label - no label for the token\n",
    "\n",
    "    def get_aligned_label_ids_from_annotations(self, tokenized_text, annotations):\n",
    "        raw_labels = align_tokens_and_annotations_bilou(tokenized_text, annotations)    \n",
    "        return list(map(self.labels_to_id.get, raw_labels))\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "@dataclass\n",
    "class TrainingExample:\n",
    "    input_ids: IntList\n",
    "    attention_masks: IntList\n",
    "    labels: IntList\n",
    "\n",
    "\n",
    "class TraingDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Any,\n",
    "        label_set: LabelSet,\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        tokens_per_batch=32,\n",
    "        window_stride=None,\n",
    "    ):\n",
    "        self.label_set = label_set\n",
    "        if window_stride is None:\n",
    "            self.window_stride = tokens_per_batch\n",
    "        self.tokenizer = tokenizer\n",
    "        for example in data:\n",
    "            # changes tag key to label\n",
    "            for a in example[\"entities\"]:\n",
    "                a[\"label\"] = a[\"tag\"]\n",
    "                pass\n",
    "        self.texts = []\n",
    "        self.annotations = []\n",
    "\n",
    "        for example in data:\n",
    "            self.texts.append(example[\"text\"])\n",
    "            self.annotations.append(example[\"entities\"])\n",
    "        ###TOKENIZE All THE DATA\n",
    "        tokenized_batch = self.tokenizer(self.texts, add_special_tokens=False)\n",
    "        ###ALIGN LABELS ONE EXAMPLE AT A TIME\n",
    "        aligned_labels = []\n",
    "        for ix in range(len(tokenized_batch.encodings)):\n",
    "            encoding = tokenized_batch.encodings[ix]\n",
    "            raw_annotations = self.annotations[ix]            \n",
    "            aligned = label_set.get_aligned_label_ids_from_annotations(\n",
    "                encoding, raw_annotations\n",
    "            )\n",
    "            aligned_labels.append(aligned)\n",
    "        ###END OF LABEL ALIGNMENT\n",
    "\n",
    "        ###MAKE A LIST OF TRAINING EXAMPLES. (This is where we add padding)\n",
    "        self.training_examples: List[TrainingExample] = []\n",
    "        empty_label_id = \"O\"\n",
    "        for encoding, label in zip(tokenized_batch.encodings, aligned_labels):\n",
    "            length = len(label)  # How long is this sequence\n",
    "            for start in range(0, length, self.window_stride):\n",
    "\n",
    "                end = min(start + tokens_per_batch, length)\n",
    "\n",
    "                # How much padding do we need ?\n",
    "                padding_to_add = max(0, tokens_per_batch - end + start)\n",
    "                self.training_examples.append(\n",
    "                    TrainingExample(\n",
    "                        # Record the tokens\n",
    "                        input_ids=encoding.ids[start:end]  # The ids of the tokens\n",
    "                        + [self.tokenizer.pad_token_id]\n",
    "                        * padding_to_add,  # padding if needed\n",
    "                        labels=(\n",
    "                            label[start:end]\n",
    "                            + [-100] * padding_to_add  # padding if needed\n",
    "                        ),  # -100 is a special token for padding of labels,\n",
    "                        attention_masks=(\n",
    "                            encoding.attention_mask[start:end]\n",
    "                            + [0]\n",
    "                            * padding_to_add  # 0'd attenetion masks where we added padding\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_examples)\n",
    "\n",
    "    def __getitem__(self, idx) -> TrainingExample:\n",
    "\n",
    "        return self.training_examples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraingingBatch:\n",
    "    def __getitem__(self, item):\n",
    "        return getattr(self, item)\n",
    "\n",
    "    def __init__(self, examples: List[TrainingExample]):\n",
    "        self.input_ids: torch.Tensor\n",
    "        self.attention_masks: torch.Tensor\n",
    "        self.labels: torch.Tensor\n",
    "        input_ids: IntListList = []\n",
    "        masks: IntListList = []\n",
    "        labels: IntListList = []\n",
    "        for ex in examples:\n",
    "            input_ids.append(ex.input_ids)\n",
    "            masks.append(ex.attention_masks)\n",
    "            labels.append(ex.labels)\n",
    "        self.input_ids = torch.LongTensor(input_ids)\n",
    "        self.attention_masks = torch.LongTensor(masks)\n",
    "        self.labels = torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = LabelSet(labels=[\"OBRA\"])\n",
    "\n",
    "ds = TraingDataset(\n",
    "    data=dataset_train, tokenizer=tokenizer, label_set=label_set, tokens_per_batch=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import RobertaForTokenClassification\n",
    "\n",
    "\n",
    "\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    \"roberta-base\", num_labels=len(ds.label_set.ids_to_label.values())\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "nb_grad_acc_steps = 4\n",
    "warmup_steps = 5\n",
    "num_epochs = 5\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ds,\n",
    "    collate_fn=TraingingBatch,\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "nb_training_steps = len(dataloader)//nb_grad_acc_steps\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,num_training_steps=nb_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def check_accuracy_model(model,dataset_raw):\n",
    "\n",
    "    outs = []\n",
    "    for d in dataset_raw:\n",
    "\n",
    "        \n",
    "            if len(d['entities'])>0:\n",
    "                y_true = d['entities'][0]['text']\n",
    "            else:\n",
    "                y_true = False\n",
    "\n",
    "            inputs = tokenizer(d['text'], return_tensors=\"pt\",truncation=True)\n",
    "\n",
    "            logits = model(**inputs)\n",
    "\n",
    "            idx = torch.nonzero(torch.argmax(logits[0][0],axis=1))\n",
    "            if (len(idx)):\n",
    "                ids_labeled = inputs.input_ids[0][idx]\n",
    "                tokens = tokenizer.convert_ids_to_tokens(ids_labeled)\n",
    "                y_pred = tokenizer.convert_tokens_to_string(tokens).strip()\n",
    "                \n",
    "            else:\n",
    "                y_pred = False\n",
    "            \n",
    "            \n",
    "            # a true prediction only counts if the model found the correct book\n",
    "            if y_pred:\n",
    "                if y_true:\n",
    "                    flag = False\n",
    "                    for s in y_pred.split():\n",
    "                        if y_true.find(s) != -1:\n",
    "                            temp = y_true.find(y_pred) != -1\n",
    "                            if(temp):\n",
    "                                flag = True\n",
    "                    y_pred = flag\n",
    "                    y_true = True\n",
    "                else:\n",
    "                    y_pred = False\n",
    "                    y_true = True\n",
    "            else:\n",
    "                if y_true:\n",
    "                    y_true = True\n",
    "            outs.append((y_true,y_pred))\n",
    "            \n",
    "                \n",
    "                \n",
    "\n",
    "    y_true = list(map(lambda x : x[0] , outs))\n",
    "    y_pred = list(map(lambda x : x[1] , outs))\n",
    "\n",
    "    return classification_report(y_true, y_pred, target_names=['positive','negative'],output_dict=True)['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/5: 100%|██████████| 295/295 [16:45<00:00,  3.41s/it, loss=0.0308] \n",
      "  0%|          | 0/295 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch val loss Goodreads:0.7200902934537246 Val Set:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=1/5: 100%|██████████| 295/295 [15:03<00:00,  3.06s/it, loss=0.0374] \n",
      "  0%|          | 0/295 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch val loss Goodreads:0.7200902934537246 Val Set:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=2/5:   1%|          | 2/295 [00:08<21:39,  4.44s/it, loss=0.0232]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-813464edd7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnb_grad_acc_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    iterator = dataloader.__iter__()\n",
    "\n",
    "    loop = tqdm(range(int(nb_training_steps)))\n",
    "    \n",
    "    for i_train in loop:\n",
    "        \n",
    "        loss=0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i_acc in range(nb_grad_acc_steps):\n",
    "            \n",
    "            batch = next(iterator)\n",
    "\n",
    "\n",
    "            loss_, logits = model(\n",
    "                input_ids=batch.input_ids,\n",
    "                attention_mask=batch.attention_masks,\n",
    "                labels=batch.labels,\n",
    "            )\n",
    "            loss+=loss_\n",
    "\n",
    "        loss=loss/nb_grad_acc_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        loop.set_description(f'Epoch={epoch}/{num_epochs}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    gd_val = check_accuracy_model(model,ds_goodreads)\n",
    "    val_val = check_accuracy_model(model,dataset_test)\n",
    "\n",
    "    print(f'Epoch val loss Goodreads:{gd_val} Val Set:{val_val}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['O', 'B-OBRA', 'I-OBRA', 'L-OBRA', 'U-OBRA'])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.label_set.ids_to_label.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import check_if_citation_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_citation_ner(model,tokenizer,'Reading The Prince and The Republic has changed my life')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_citation_ner(model,tokenizer,'The prince reads Marcus Aurelius\\' Meditations to relax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_citation_ner(model,tokenizer,\"When Odysseus returns to Ithaca in Book 13 of The Odyssey, Athena disguises him as an old beggar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_citation_ner(model,tokenizer,\"But these more outré materials combine to form what is finally a thematic second tier. The foreground of Infinite Jest features three basic plot systems. At the center of one is Hal Incandenza, an adolescent tennis star attending Enfield Tennis Academy (ETA), which his family founded,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_citation_ner(model,tokenizer,\"the commencement of war a herald might be called upon to recite the causes of the conflict; in effect, to provide the motivation. In Shakespeare's Henry IV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_citation_ner(model,tokenizer,\"is impersonal in the Meditations agrees closely with Epictetus. Marcus Aurelius is doubtful about immortality, but says, as a Christian might: 'Since it is possible th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_citation_ner(model,tokenizer,\"8 Melville’s Moby-Dick, for instance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model.save_pretrained(f\"fine-tuned-model-ner-better-data-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "[{'start': 18, 'end': 37, 'text': 'Nicomachaean Ethics', 'label': 'OBRA', 'tag': 'OBRA'}] Nicomachaean Ethics\n",
      "[{'start': 3, 'end': 23, 'text': 'Occidental Mythology', 'label': 'OBRA', 'tag': 'OBRA'}] Occidental Mythology\n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[{'start': 14, 'end': 32, 'text': 'Nicomachean Ethics', 'label': 'OBRA', 'tag': 'OBRA'}] Nicomachean Ethics\n",
      "[{'start': 13, 'end': 24, 'text': 'The Odyssey', 'label': 'OBRA', 'tag': 'OBRA'}] The Odyssey\n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[{'start': 14, 'end': 33, 'text': 'Nicomachaean Ethics', 'label': 'OBRA', 'tag': 'OBRA'}] Nicomachaean Ethics\n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[{'start': 16, 'end': 35, 'text': 'Nicomachaean Ethics', 'label': 'OBRA', 'tag': 'OBRA'}] Nicomachaean Ethics\n",
      "[{'start': 296, 'end': 302, 'text': 'Ethics', 'label': 'OBRA', 'tag': 'OBRA'}] Ethics\n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[{'start': 539, 'end': 559, 'text': 'Occidental Mythology', 'label': 'OBRA', 'tag': 'OBRA'}] Occidental Mythology\n",
      "[{'start': 95, 'end': 101, 'text': 'Hamlet', 'label': 'OBRA', 'tag': 'OBRA'}] Hamlet\n",
      "[] \n",
      "[] \n",
      "[{'start': 224, 'end': 231, 'text': 'Macbeth', 'label': 'OBRA', 'tag': 'OBRA'}] Macbeth\n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] \n",
      "[] Ham\n",
      "[{'start': 17, 'end': 23, 'text': 'Ethics', 'label': 'OBRA', 'tag': 'OBRA'}] Ethics\n",
      "[{'start': 9, 'end': 29, 'text': 'Occidental Mythology', 'label': 'OBRA', 'tag': 'OBRA'}] Occidental Mythology\n",
      "[] Hamlet\n",
      "[] \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-3c8de272fb67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_if_citation_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/OneDrive/Projetos/book-graph/utils.py\u001b[0m in \u001b[0;36mcheck_if_citation_ner\u001b[0;34m(model, tokenizer, sentence)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m         )\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         embedding_output = self.embeddings(\n\u001b[0;32m--> 684\u001b[0;31m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m         )\n\u001b[1;32m    686\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "for d in dataset_test:\n",
    "    \n",
    "    w = check_if_citation_ner(model,tokenizer,d['text'])\n",
    "    print(d['entities'],w,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 22896,\n",
       " 'text': 'To connect the force which produces the phenomenon of the world, and consequently determines its nature, with the morality of the disposition or character, and thus to establish a moralorder of the world as the foundation of the physical,—this has been since Socrates the problem of philosophy. Theism solved it in a childish manner, which could not satisfy mature humanity. Therefore pantheism opposed itself to it whenever it ventured to do so, and showed that nature bears in itself the power by virtue of which it appears. With this, however, ethics had necessarily to be given up. Spinoza, indeed, attempts here and there to preserve it by means of sophistry, but for the most part gives it up altogether, and, with a boldness which excites astonishment and repugnance, explains the distinction between right and wrong, and in general between good and evil, as merely conventional, thus in itself empty (for example, Eth. iv., prop. 37, schol. 2). After having met with unmerited neglect for more than a hundred years, Spinoza has, in general, become too much esteemed in this century through the reaction caused by the swing of the pendulum of opinion. All pantheism must ultimately be overthrown by the inevitable demands of ethics, and then by the evil and suffering of the world. If the world is a theophany, then all that man, or even [pg 404]the brute, does is equally divine and excellent; nothing can be censurable, and nothing more praiseworthy than the rest: thus there is no ethics. Hence, in consequence of the revived Spinozism of our own day, thus of pantheism, the treatment of ethics has sunk so low and become so shallow that it has been made a mere instruction as to the proper life of a citizen and a member of a family, in which the ultimate end of human existence is supposed to consist: thus in methodical, complete, smug, and comfortable philistinism. Pantheism, indeed, has only led to such shallow vulgarisms through the fact that (by a shameful misuse of the e quovis ligno fit Mercurius) a common mind, Hegel, has, by the well-known means, been falsely stamped as a great philosopher, and a herd of his disciples, at first suborned, afterwards only stupid, received his weighty words. Such outrages on the human mind do not remain unpunished: the seed has sprouted. In the same spirit it was then asserted that ethics should have for its material not the conduct of individuals, but that of nations, that this alone was a theme worthy of it. Nothing can be more perverse than this view, which rests on the most vulgar realism. For in every individual appears the whole undivided will to live, the thing in itself, and the microcosm is like the macrocosm. The masses have no more content than each individual. Ethics is concerned not with actions and their results, but with willing, and willing itself takes place only in the individual.',\n",
       " 'meta': {'book': 'Ethics'},\n",
       " 'annotation_approver': None,\n",
       " 'labels': [[2741, 2747, 'OBRA']],\n",
       " 'entities': [{'start': 2741,\n",
       "   'end': 2747,\n",
       "   'text': 'Ethics',\n",
       "   'label': 'OBRA',\n",
       "   'tag': 'OBRA'}]}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lira/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lira/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel,RobertaForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForTokenClassification.from_pretrained('fine-tuned-model-ner-better-data-2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_if_citation_ner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d890b596be60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_if_citation_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'So Plato reads The Republic and then replies to Thrasymachus and Callicles, and to all Nietzscheans forever: Justice is not mere strength, but harmonious strength—desires and men falling into that order which constitutes intelligence and organization; justice is not the right of the stronger, but the effective harmony of the whole. It is true that the individual who gets out of the place to which his nature and talents adapt him may for a time seize some profit and advantage; but an inescapable [[BOOK]] pursues him—as Anaxagoras spoke of the Furies pursuing any planet that should wander out of its orbit; the terrible baton of the Nature of Things drives the refractory instrument back to its place and its pitch and its natural note.\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'check_if_citation_ner' is not defined"
     ]
    }
   ],
   "source": [
    "check_if_citation_ner(model,tokenizer,'So Plato reads The Republic and then replies to Thrasymachus and Callicles, and to all Nietzscheans forever: Justice is not mere strength, but harmonious strength—desires and men falling into that order which constitutes intelligence and organization; justice is not the right of the stronger, but the effective harmony of the whole. It is true that the individual who gets out of the place to which his nature and talents adapt him may for a time seize some profit and advantage; but an inescapable [[BOOK]] pursues him—as Anaxagoras spoke of the Furies pursuing any planet that should wander out of its orbit; the terrible baton of the Nature of Things drives the refractory instrument back to its place and its pitch and its natural note.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('annotated/unused/dataset_annotated_doccano_calibre.jsonl','r') as f:\n",
    "    \n",
    "    ds_ = list(map(lambda x : json.loads(x),f.readlines()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "illegal target for annotation (<ipython-input-80-15a66747d392>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-80-15a66747d392>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    \"entities\": [{\"start\": 4, \"end\": 25, \"text\": \"Shakespeare's Sonnets\", \"label\": \"OBRA\", \"tag\": \"OBRA\"}]\u001b[0m\n\u001b[0m                                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m illegal target for annotation\n"
     ]
    }
   ],
   "source": [
    "\"entities\": [{\"start\": 4, \"end\": 25, \"text\": \"Shakespeare's Sonnets\", \"label\": \"OBRA\", \"tag\": \"OBRA\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to non-doccano format\n",
    "for d in ds_:\n",
    "    d['entities'] = []\n",
    "    \n",
    "    if (d['labels']): \n",
    "        for l in d['labels']:\n",
    "            b =  l[0]\n",
    "            e = l[1]\n",
    "\n",
    "            book = d['text'][b:e]\n",
    "\n",
    "            entities = {\n",
    "                'start' : b,\n",
    "                'end' : e,\n",
    "                'text' : book,\n",
    "                'label' : \"OBRA\",\n",
    "                'tag' : \"OBRA\"\n",
    "            }\n",
    "            d['entities'].append(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotated/dataset_annotated-fixed-done.jsonl','w') as f:\n",
    "    for js in ds_: \n",
    "        f.write(json.dumps(js) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for d in ds_:\n",
    "    book = d['meta']['book']\n",
    "    \n",
    "    temp = d['text'].replace('[[BOOK]]',book)\n",
    "    d['text'] = temp\n",
    "    b = temp.find(book)\n",
    "    e = b + len(book)\n",
    "    if(d['labels']==['negative']):\n",
    "        del d['entities']\n",
    "        d['labels']=[]\n",
    "    else:\n",
    "        #dict(start=5,end=14,text=\"Tal Perry\",label=\"Person\")\n",
    "        entities = {\n",
    "            'start' : b,\n",
    "            'end' : e,\n",
    "            'text' : book,\n",
    "            'label' : \"OBRA\",\n",
    "            'tag' : \"OBRA\"\n",
    "        }\n",
    "        d['labels'] = [[b,e,\"OBRA\"]]\n",
    "        #d['entities'] = [entities]\n",
    "        del d['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotated/dataset_annotated-to_fix-doccano-format.jsonl','w') as f:\n",
    "    for js in ds_: \n",
    "        f.write(json.dumps(js) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "book = \"The Republic\"\n",
    "\n",
    "inputs = tokenizer(\"Read The Republic and Moby-Dick\", return_tensors=\"pt\",truncation=True)\n",
    "\n",
    "logits = model(**inputs)\n",
    "\n",
    "idx = torch.nonzero(torch.argmax(logits[0][0],axis=1))\n",
    "if (len(idx)):\n",
    "    ids_labeled = inputs.input_ids[0][idx]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids_labeled)\n",
    "    y_pred = tokenizer.convert_tokens_to_string(tokens).strip()\n",
    "\n",
    "for word in y_pred.split():\n",
    "    if book.find(word):\n",
    "        return True\n",
    "return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Republic', 'MobyDick']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.split()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
